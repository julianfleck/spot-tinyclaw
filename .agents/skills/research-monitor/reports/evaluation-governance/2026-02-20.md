# Evaluation & Governance — Week of Feb 20, 2026

Memory benchmarks for agents are emerging rapidly, moving evaluation from static QA to dynamic, multi-session memory retention and streaming lifecycle management.

---

## Highlights

### MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments
**Liu et al.** · Multiple institutions · [arxiv](https://arxiv.org/abs/2602.06075)

Current mobile GUI agent benchmarks systematically fail to assess memory: only 5.2-11.8% of tasks involve memory, with zero cross-session learning evaluation. MemGUI-Bench introduces 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention. Includes MemGUI-Eval, an automated evaluation pipeline with Progressive Scrutiny and 7 hierarchical metrics. Evaluation of 11 state-of-the-art agents reveals significant memory deficits across all systems, identifying 5 distinct failure modes and synthesizing 5 actionable design implications.

**RAGE implication:** Frame traversal across sessions requires persistent memory with cross-temporal consistency — current GUI agents fail exactly this test.

*Relevance: high*

---

### Neuromem: A Granular Decomposition of the Streaming Lifecycle in External Memory for LLMs
**Zhang et al.** · [arxiv](https://arxiv.org/abs/2602.13967)

Most External Memory Module evaluations assume a static setting: memory built offline, queried at fixed state. Neuromem benchmarks memory under streaming conditions — new facts arrive continuously, insertions interleave with retrievals, memory state evolves during serving. Decomposes lifecycle into five dimensions: data structure, normalization strategy, consolidation policy, query formulation, and context integration. Finds that performance degrades as memory grows across rounds, time-related queries remain hardest, and aggressive compression mostly shifts cost between insertion/retrieval with limited accuracy gain.

**RAGE implication:** RAGE's frame lifecycle (update, consolidate, query) maps directly to Neuromem's dimensions — validates streaming evaluation as critical.

*Relevance: high*

---

### What Makes a Good LLM Agent for Real-world Penetration Testing?
**Deng et al.** · [arxiv](https://arxiv.org/abs/2602.17622)

Analysis of 28 LLM-based penetration testing systems reveals two failure modes: Type A (missing tools, fixable via engineering) and Type B (planning/state management failures invariant to tooling or model). Type B stems from agents lacking real-time task difficulty estimation — they misallocate effort, over-commit to low-value branches, exhaust context. Excalibur introduces Task Difficulty Assessment (TDA) via four dimensions (horizon estimation, evidence confidence, context load, historical success) plus Evidence-Guided Attack Tree Search (EGATS). Achieves 91% task completion on CTF benchmarks (39-49% relative improvement) and compromises 4/5 hosts on GOAD AD environment vs. 2 for prior systems.

**RAGE implication:** Difficulty estimation for frame traversal — agents need to assess tractability before committing to exploration paths.

*Relevance: high*

---

## Scan

| Paper | Institution | Focus | Gist |
|-------|-------------|-------|------|
| [EMemBench](https://arxiv.org/abs/2601.16690) | Multiple | episodic memory benchmark | Programmatic benchmark for VLM agent memory via interactive games; induction and spatial reasoning are bottlenecks |
| [Multi-Session Agent Memory](https://arxiv.org/abs/2602.16313) | — | interdependent tasks | Benchmarks memory across multi-session agentic tasks; compares MemTrack, EMemBench, AgentLongBench |
| [Memory Needs Graphs?](https://arxiv.org/abs/2601.01280) | — | graph vs. other structures | Evaluates Update and Noop operations on HaluMem benchmark for memory extraction |
| [MemCtrl](https://arxiv.org/abs/2601.20831) | — | active memory control | MLLMs as memory controllers for embodied agents; 16% avg improvement on EmbodiedBench |

---

**Keywords:** memory benchmark LLM, hallucination evaluation agents, agent security vulnerabilities, long-horizon evaluation, MemoryBench, HaluMem, MEMTRACK benchmark

**Sources:** brave (rate-limited after 2 queries), arxiv

**Period:** Feb 13-20, 2026

**Next:** Friday, Feb 27, 2026
